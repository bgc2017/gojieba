jieba


=
=
=
=
=
=
=
=


“
结巴
”
中文
分词
：
做
最好
的
 
Python
 
中文
分词
组件




"
Jieba
"
 
(
Chinese
 
for
 
"
to
 
stutter
"
)
 
Chinese
 
text
 
segmentation
:
 
built
 
to
 
be
 
the
 
best
 
Python
 
Chinese
 
word
 
segmentation
 
module
.




-
 
_
Scroll
 
down
 
for
 
English
 
documentation
._






特点


=
=
=
=
=
=
=
=


*
 
支持
四种
分
模式
词模式
：


 
 
 
 
*
 
精确
模式
，
试图
将
句子
最
精确
地
切开
，
适合
文本
分析
；


 
 
 
 
*
 
全
模式
，
把
句子
中
所有
的
可以
成词
的
词语
都
扫描
出来
,
 
速度
非常
快
，
但是
不能
解决
歧义
；


 
 
 
 
*
 
搜索
索引
引擎
搜索引擎
模式
，
在
精确
模式
的
基础
上
，
对长
词
再次
切分
，
提高
召回
率
，
适合
用于
搜索
索引
引擎
搜索引擎
分词
。


 
 
 
 
*
 
paddle
模式
，
利用
PaddlePaddle
深度
学习
框架
，
训练
序列
标注
（
双向
GRU
）
网络
模型
实现
分词
。
同时
支持
词性
标注
。
paddle
模式
使用
需
安装
paddlepaddle
-
tiny
，
`
pip
 
install
 
paddlepaddle
-
tiny
=
=
1.6
.
1
`
。
目前
paddle
模式
支持
jieba
 
v0.40
及
以上
版本
。
jieba
 
v0.40
以下
版本
，
请
升级
jieba
，
`
pip
 
install
 
jieba
 
--
upgrade
`
 
。
[
PaddlePaddle
官网
]
(
https
:
/
/
www
.
paddlepaddle
.
org
.
cn
/
)


*
 
支持
繁体
分词


*
 
支持
自定
定义
自定义
词典


*
 
MIT
 
授权
协议




安装
说明


=
=
=
=
=
=
=




代码
对
 
Python
 
2
/
3
 
均
兼容




*
 
自动
全自动
安装
：
`
easy
_
install
 
jieba
`
 
或者
 
`
pip
 
install
 
jieba
`
 
/
 
`
pip3
 
install
 
jieba
`


*
 
自动
半自动
安装
：
先
下载
 
http
:
/
/
pypi
.
python
.
org
/
pypi
/
jieba
/
 
，
解压
后
运行
 
`
python
 
setup
.
py
 
install
`


*
 
手动
安装
：
将
 
jieba
 
目录
放置
于
当前
目录
当前目录
或者
 
site
-
packages
 
目录


*
 
通过
 
`
import
 
jieba
`
 
来
引用


*
 
如果
需要
使用
paddle
模式
下
的
分词
和
词性
标注
功能
，
请
先
安装
paddlepaddle
-
tiny
，
`
pip
 
install
 
paddlepaddle
-
tiny
=
=
1.6
.
1
`
。




算法


=
=
=
=
=
=
=
=


*
 
基于
前缀
词典
实现
高效
的
词图
扫描
，
生成
句子
中
汉字
所有
可能
成词
情况
所
构成
的
有
向
无
环图
 
(
DAG
)


*
 
采用
了
动态
规划
查找
最大
概率
路径
,
 
找出
基于
词频
的
最大
切分
组合


*
 
对于
未
登录
词
，
采用
了
基于
汉字
成词
能力
的
 
HMM
 
模型
，
使用
了
 
Viterbi
 
算法




主要
功能


=
=
=
=
=
=
=


1
.
 
分词


--------


*
 
`
jieba
.
cut
`
 
方法
接受
四个
输入
参数
:
 
需要
分词
的
字符
字符串
；
cut
_
all
 
参数
用来
控制
是否
采用
全
模式
；
HMM
 
参数
用来
控制
是否
使用
 
HMM
 
模型
；
use
_
paddle
 
参数
用来
控制
是否
使用
paddle
模式
下
的
分
模式
词模式
，
paddle
模式
采用
延迟
加载
方式
，
通过
enable
_
paddle
接口
安装
paddlepaddle
-
tiny
，
并且
import
相关
代码
；


*
 
`
jieba
.
cut
_
for
_
search
`
 
方法
接受
两个
参数
：
需要
分词
的
字符
字符串
；
是否
使用
 
HMM
 
模型
。
该
方法
适合
用于
搜索
索引
引擎
搜索引擎
构建
倒排
索引
的
分词
，
粒度
比较
细


*
 
待
分词
的
字符
字符串
可以
是
 
unicode
 
或
 
UTF
-
8
 
字符
字符串
、
GBK
 
字符
字符串
。
注意
：
不
建议
直接
输入
 
GBK
 
字符
字符串
，
可能
无法
预料
地
错误
解码
成
 
UTF
-
8


*
 
`
jieba
.
cut
`
 
以及
 
`
jieba
.
cut
_
for
_
search
`
 
返回
的
结构
都
是
一个
可
迭代
的
 
generator
，
可以
使用
 
for
 
循环
来
获得
分词
后
得到
的
每
一个
词语
(
unicode
)
，
或者
用


*
 
`
jieba
.
lcut
`
 
以及
 
`
jieba
.
lcut
_
for
_
search
`
 
直接
返回
 
list


*
 
`
jieba
.
Tokenizer
(
dictionary
=
DEFAULT
_
DICT
)
`
 
新建
自定
定义
自定义
分词
分词器
，
可
用于
同时
使用
不同
词典
。
`
jieba
.
dt
`
 
为
默认
分词
分词器
，
所有
全局
分词
相关
函数
都
是
该
分词
分词器
的
映射
。




代码
示例




`
`
`
python


#
 
encoding
=
utf
-
8


import
 
jieba




jieba
.
enable
_
paddle
(
)
#
 
启动
paddle
模式
。
 
0.40
版
之后
开始
支持
，
早期
版本
不
支持


strs
=
[
"
我
来到
北京
清华
华大
大学
清华大学
"
,
"
乒乓
乒乓球
拍卖
完
了
"
,
"
中国
科学
技术
科学技术
大学
"
]


for
 
str
 
in
 
strs
:


 
 
 
 
seg
_
list
 
=
 
jieba
.
cut
(
str
,
use
_
paddle
=
True
)
 
#
 
使用
paddle
模式


 
 
 
 
print
(
"
Paddle
 
Mode
:
 
"
 
+
 
'
/
'
.
join
(
list
(
seg
_
list
)
)
)




seg
_
list
 
=
 
jieba
.
cut
(
"
我
来到
北京
清华
华大
大学
清华大学
"
,
 
cut
_
all
=
True
)


print
(
"
Full
 
Mode
:
 
"
 
+
 
"
/
 
"
.
join
(
seg
_
list
)
)
 
 
#
 
全
模式




seg
_
list
 
=
 
jieba
.
cut
(
"
我
来到
北京
清华
华大
大学
清华大学
"
,
 
cut
_
all
=
False
)


print
(
"
Default
 
Mode
:
 
"
 
+
 
"
/
 
"
.
join
(
seg
_
list
)
)
 
 
#
 
精确
模式




seg
_
list
 
=
 
jieba
.
cut
(
"
他
来到
了
网易
杭研
大厦
"
)
 
 
#
 
默认
是
精确
模式


print
(
"
,
 
"
.
join
(
seg
_
list
)
)




seg
_
list
 
=
 
jieba
.
cut
_
for
_
search
(
"
小明
硕士
毕业
于
中国
科学
学院
科学院
中国科学院
计算
计算所
，
后
在
日本
京都
大学
日本京都大学
深造
"
)
 
 
#
 
搜索
索引
引擎
搜索引擎
模式


print
(
"
,
 
"
.
join
(
seg
_
list
)
)


`
`
`




输出
:




 
 
 
 
【
全
模式
】
:
 
我
/
 
来到
/
 
北京
/
 
清华
/
 
清华
华大
大学
清华大学
/
 
华大
/
 
大学




 
 
 
 
【
精确
模式
】
:
 
我
/
 
来到
/
 
北京
/
 
清华
华大
大学
清华大学




 
 
 
 
【
新词
识别
】
：
他
,
 
来到
,
 
了
,
 
网易
,
 
杭研
,
 
大厦
 
 
 
 
(
此处
，
“
杭研
”
并
没有
在
词典
中
，
但是
也
被
Viterbi
算法
识别
出来
了
)




 
 
 
 
【
搜索
索引
引擎
搜索引擎
模式
】
：
 
小明
,
 
硕士
,
 
毕业
,
 
于
,
 
中国
,
 
科学
,
 
学院
,
 
科学
学院
科学院
,
 
中国
科学
学院
科学院
中国科学院
,
 
计算
,
 
计算
计算所
,
 
后
,
 
在
,
 
日本
,
 
京都
,
 
大学
,
 
日本
京都
大学
日本京都大学
,
 
深造




2
.
 
添加
自定
定义
自定义
词典


----------------




###
 
载入
词典




*
 
开发
开发者
可以
指定
自己
自定
定义
自定义
的
词典
，
以便
包含
 
jieba
 
词
库里
没有
的
词
。
虽然
 
jieba
 
有
新词
识别
能力
，
但是
自行
添加
自行添加
新词
可以
保证
更
高
的
正确
正确率


*
 
用法
：
 
jieba
.
load
_
userdict
(
file
_
name
)
 
#
 
file
_
name
 
为
文件
类
对象
或
自定
定义
自定义
词典
的
路径


*
 
词典
格式
和
 
`
dict
.
txt
`
 
一样
，
一个
词
占
一行
；
每
一行
分三
部分
：
词语
、
词频
（
可
省略
）
、
词性
（
可
省略
）
，
用
空格
隔开
，
顺序
不可
颠倒
。
`
file
_
name
`
 
若
为
路径
或
进制
二进制
方式
打开
的
文件
，
则
文件
必须
为
 
UTF
-
8
 
编码
。


*
 
词频
省略
时
使用
自动
计算
的
能
保证
分出
该词
的
词频
。




*
*
例如
：
*
*




`
`
`


创新
办
 
3
 
i


云
计算
 
5


凱特琳
 
nz


台
中


`
`
`




*
 
更改
分词
分词器
（
默认
为
 
`
jieba
.
dt
`
）
的
 
`
tmp
_
dir
`
 
和
 
`
cache
_
file
`
 
属性
，
可
分别
指定
缓存
文件
所在
的
文件
件夹
文件夹
及其
文件
文件名
，
用于
受限
的
文件
系统
文件系统
。




*
 
范例
：




 
 
 
 
*
 
自定
定义
自定义
词典
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
userdict
.
txt




 
 
 
 
*
 
用法
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
test
_
userdict
.
py






 
 
 
 
 
 
 
 
*
 
之前
：
 
李小福
 
/
 
是
 
/
 
创新
 
/
 
办
 
/
 
主任
 
/
 
也
 
/
 
是
 
/
 
云
 
/
 
计算
 
/
 
方面
 
/
 
的
 
/
 
专家
 
/




 
 
 
 
 
 
 
 
*
 
加载
自定
定义
自定义
自定义词
库后
：
　
李小福
 
/
 
是
 
/
 
创新
办
 
/
 
主任
 
/
 
也
 
/
 
是
 
/
 
云
计算
 
/
 
方面
 
/
 
的
 
/
 
专家
 
/




###
 
调整
词典




*
 
使用
 
`
add
_
word
(
word
,
 
freq
=
None
,
 
tag
=
None
)
`
 
和
 
`
del
_
word
(
word
)
`
 
可
在
程序
中
动态
修改
词典
。


*
 
使用
 
`
suggest
_
freq
(
segment
,
 
tune
=
True
)
`
 
可
调节
单个
词语
的
词频
，
使
其能
（
或
不能
）
被
分
出来
。




*
 
注意
：
自动
计算
的
词频
在
使用
 
HMM
 
新词
发现
功能
时
可能
无效
。




代码
示例
：




`
`
`
pycon


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
如果
放到
post
中将
出错
。
'
,
 
HMM
=
False
)
)
)


如果
/
放到
/
post
/
中将
/
出错
/
。


>
>
>
 
jieba
.
suggest
_
freq
(
(
'
中
'
,
 
'
将
'
)
,
 
True
)


494


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
如果
放到
post
中将
出错
。
'
,
 
HMM
=
False
)
)
)


如果
/
放到
/
post
/
中
/
将
/
出错
/
。


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
「
台
中
」
正确
应该
不会
被
切开
'
,
 
HMM
=
False
)
)
)


「
/
台
/
中
/
」
/
正确
/
应该
/
不会
/
被
/
切开


>
>
>
 
jieba
.
suggest
_
freq
(
'
台
中
'
,
 
True
)


69


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
「
台
中
」
正确
应该
不会
被
切开
'
,
 
HMM
=
False
)
)
)


「
/
台
中
/
」
/
正确
/
应该
/
不会
/
被
/
切开


`
`
`




*
 
"
通过
用户
自定
定义
自定义
词典
来
增强
歧义
纠错
能力
"
 
---
 
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
issues
/
14




3
.
 
关键
关键词
提取


-------------


###
 
基于
 
TF
-
IDF
 
算法
的
关键
关键词
抽取




`
import
 
jieba
.
analyse
`




*
 
jieba
.
analyse
.
extract
_
tags
(
sentence
,
 
topK
=
20
,
 
withWeight
=
False
,
 
allowPOS
=
(
)
)


 
 
*
 
sentence
 
为
待
提取
的
文本


 
 
*
 
topK
 
为
返回
几个
 
TF
/
IDF
 
权重
最大
的
关键
关键词
，
默认
默认值
为
 
20


 
 
*
 
withWeight
 
为
是否
一并
返回
关键
关键词
权重
值
，
默认
默认值
为
 
False


 
 
*
 
allowPOS
 
仅
包括
指定
词性
的
词
，
默认
默认值
为空
，
即
不
筛选


*
 
jieba
.
analyse
.
TFIDF
(
idf
_
path
=
None
)
 
新建
 
TFIDF
 
实例
，
idf
_
path
 
为
 
IDF
 
频率
文件




代码
示例
 
（
关键
关键词
提取
）




https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
.
py




关键
关键词
提取
所
使用
逆向
文件
频率
（
IDF
）
文本
语料
语料库
可以
切换
成
自定
定义
自定义
语料
语料库
的
路径




*
 
用法
：
 
jieba
.
analyse
.
set
_
idf
_
path
(
file
_
name
)
 
#
 
file
_
name
为
自定
定义
自定义
语料
语料库
的
路径


*
 
自定
定义
自定义
语料
语料库
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
extra
_
dict
/
idf
.
txt
.
big


*
 
用法
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
_
idfpath
.
py




关键
关键词
提取
所
使用
停止
词
（
Stop
 
Words
）
文本
语料
语料库
可以
切换
成
自定
定义
自定义
语料
语料库
的
路径




*
 
用法
：
 
jieba
.
analyse
.
set
_
stop
_
words
(
file
_
name
)
 
#
 
file
_
name
为
自定
定义
自定义
语料
语料库
的
路径


*
 
自定
定义
自定义
语料
语料库
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
extra
_
dict
/
stop
_
words
.
txt


*
 
用法
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
_
stop
_
words
.
py




关键
关键词
一并
返回
关键
关键词
权重
值
示例




*
 
用法
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
_
with
_
weight
.
py




###
 
基于
 
TextRank
 
算法
的
关键
关键词
抽取




*
 
jieba
.
analyse
.
textrank
(
sentence
,
 
topK
=
20
,
 
withWeight
=
False
,
 
allowPOS
=
(
'
ns
'
,
 
'
n
'
,
 
'
vn
'
,
 
'
v
'
)
)
 
直接
使用
，
接口
相同
，
注意
默认
过滤
词性
。


*
 
jieba
.
analyse
.
TextRank
(
)
 
新建
自定
定义
自定义
 
TextRank
 
实例




算法
论文
：
 
[
TextRank
:
 
Bringing
 
Order
 
into
 
Texts
]
(
http
:
/
/
web
.
eecs
.
umich
.
edu
/
~
mihalcea
/
papers
/
mihalcea
.
emnlp04
.
pdf
)




####
 
基本
思想
:




1
.
 
将
待
抽取
关键
关键词
的
文本
进行
分词


2
.
 
以
固定
窗口
大小
(
默认
为
5
，
通过
span
属性
调整
)
，
词
之间
的
共现
关系
，
构建
图


3
.
 
计算
图中
节点
的
PageRank
，
注意
是
无
向
带
权图




####
 
使用
示例
:




见
 
[
test
/
demo
.
py
]
(
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
demo
.
py
)




4
.
 
词性
标注


-----------


*
 
`
jieba
.
posseg
.
POSTokenizer
(
tokenizer
=
None
)
`
 
新建
自定
定义
自定义
分词
分词器
，
`
tokenizer
`
 
参数
可
指定
内部
使用
的
 
`
jieba
.
Tokenizer
`
 
分词
分词器
。
`
jieba
.
posseg
.
dt
`
 
为
默认
词性
标注
分词
分词器
。


*
 
标注
句子
分词
后
每个
词
的
词性
，
采用
和
 
ictclas
 
兼容
的
标记
法
。


*
 
除了
jieba
默认
分
模式
词模式
，
提供
paddle
模式
下
的
词性
标注
功能
。
paddle
模式
采用
延迟
加载
方式
，
通过
enable
_
paddle
(
)
安装
paddlepaddle
-
tiny
，
并且
import
相关
代码
；


*
 
用法
示例




`
`
`
pycon


>
>
>
 
import
 
jieba


>
>
>
 
import
 
jieba
.
posseg
 
as
 
pseg


>
>
>
 
words
 
=
 
pseg
.
cut
(
"
我
爱
北京
天安
天安门
"
)
 
#
jieba
默认
模式


>
>
>
 
jieba
.
enable
_
paddle
(
)
 
#
启动
paddle
模式
。
 
0.40
版
之后
开始
支持
，
早期
版本
不
支持


>
>
>
 
words
 
=
 
pseg
.
cut
(
"
我
爱
北京
天安
天安门
"
,
use
_
paddle
=
True
)
 
#
paddle
模式


>
>
>
 
for
 
word
,
 
flag
 
in
 
words
:


...
 
 
 
 
print
(
'
%
s
 
%
s
'
 
%
 
(
word
,
 
flag
)
)


...


我
 
r


爱
 
v


北京
 
ns


天安
天安门
 
ns


`
`
`




paddle
模式
词性
标注
对应
表
如下
：




paddle
模式
词性
和
专名
类别
标签
集合
如下
表
，
其中
词性
标签
 
24
 
个
（
小写
写字
字母
小写字母
）
，
专名
类别
标签
 
4
 
个
（
大写
写字
字母
大写字母
）
。




|
 
标签
 
|
 
含义
 
 
 
 
 
|
 
标签
 
|
 
含义
 
 
 
 
 
|
 
标签
 
|
 
含义
 
 
 
 
 
|
 
标签
 
|
 
含义
 
 
 
 
 
|


|
 
----
 
|
 
--------
 
|
 
----
 
|
 
--------
 
|
 
----
 
|
 
--------
 
|
 
----
 
|
 
--------
 
|


|
 
n
 
 
 
 
|
 
普通
名词
 
|
 
f
 
 
 
 
|
 
方位
名词
 
|
 
s
 
 
 
 
|
 
处所
名词
 
|
 
t
 
 
 
 
|
 
时间
 
 
 
 
 
|


|
 
nr
 
 
 
|
 
人
名
 
 
 
 
 
|
 
ns
 
 
 
|
 
地名
 
 
 
 
 
|
 
nt
 
 
 
|
 
机构
名
 
 
 
|
 
nw
 
 
 
|
 
作品
名
 
 
 
|


|
 
nz
 
 
 
|
 
其他
专名
 
|
 
v
 
 
 
 
|
 
普通
动词
 
|
 
vd
 
 
 
|
 
动
副词
 
 
 
|
 
vn
 
 
 
|
 
名
动词
 
 
 
|


|
 
a
 
 
 
 
|
 
形容
形容词
 
 
 
|
 
ad
 
 
 
|
 
副
形词
 
 
 
|
 
an
 
 
 
|
 
名形
词
 
 
 
|
 
d
 
 
 
 
|
 
副词
 
 
 
 
 
|


|
 
m
 
 
 
 
|
 
数量
量词
数量词
 
 
 
|
 
q
 
 
 
 
|
 
量词
 
 
 
 
 
|
 
r
 
 
 
 
|
 
代词
 
 
 
 
 
|
 
p
 
 
 
 
|
 
介词
 
 
 
 
 
|


|
 
c
 
 
 
 
|
 
连词
 
 
 
 
 
|
 
u
 
 
 
 
|
 
助词
 
 
 
 
 
|
 
xc
 
 
 
|
 
其他
虚词
 
|
 
w
 
 
 
 
|
 
标点
符号
标点符号
 
|


|
 
PER
 
 
|
 
人
名
 
 
 
 
 
|
 
LOC
 
 
|
 
地名
 
 
 
 
 
|
 
ORG
 
 
|
 
机构
名
 
 
 
|
 
TIME
 
|
 
时间
 
 
 
 
 
|






5
.
 
并行
分词


-----------


*
 
原理
：
将
目标
文本
按行
分隔
后
，
把
各行
文本
分配
到
多个
 
Python
 
进程
并行
分词
，
然后
归并
结果
，
从而
获得
分词
速度
的
可观
提升


*
 
基于
 
python
 
自带
的
 
multiprocessing
 
模块
，
目前
暂
不
支持
 
Windows


*
 
用法
：


 
 
 
 
*
 
`
jieba
.
enable
_
parallel
(
4
)
`
 
#
 
开启
并行
分
模式
词模式
，
参数
为
并行
进程
数


 
 
 
 
*
 
`
jieba
.
disable
_
parallel
(
)
`
 
#
 
关闭
并行
分
模式
词模式




*
 
例子
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
parallel
/
test
_
file
.
py




*
 
实验
结果
：
在
 
4
 
核
 
3.4
GHz
 
Linux
 
机器
上
，
对
金庸
全集
进行
精确
分词
，
获得
了
 
1MB
/
s
 
的
速度
，
是
单
进程
版
的
 
3.3
 
倍
。




*
 
*
*
注意
*
*
：
并行
分词
仅
支持
默认
分词
分词器
 
`
jieba
.
dt
`
 
和
 
`
jieba
.
posseg
.
dt
`
。




6
.
 
Tokenize
：
返回
词语
在
原文
的
起止
位置


----------------------------------


*
 
注意
，
输入
参数
只
接受
 
unicode


*
 
默认
模式




`
`
`
python


result
 
=
 
jieba
.
tokenize
(
u
'
永和
服装
饰品
有限
公司
有限公司
'
)


for
 
tk
 
in
 
result
:


 
 
 
 
print
(
"
word
 
%
s
\
t
\
t
 
start
:
 
%
d
 
\
t
\
t
 
end
:
%
d
"
 
%
 
(
tk
[
0
]
,
tk
[
1
]
,
tk
[
2
]
)
)


`
`
`




`
`
`


word
 
永和
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
0
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
2


word
 
服装
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
4


word
 
饰品
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
4
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
6


word
 
有限
公司
有限公司
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
10




`
`
`




*
 
搜索
模式




`
`
`
python


result
 
=
 
jieba
.
tokenize
(
u
'
永和
服装
饰品
有限
公司
有限公司
'
,
 
mode
=
'
search
'
)


for
 
tk
 
in
 
result
:


 
 
 
 
print
(
"
word
 
%
s
\
t
\
t
 
start
:
 
%
d
 
\
t
\
t
 
end
:
%
d
"
 
%
 
(
tk
[
0
]
,
tk
[
1
]
,
tk
[
2
]
)
)


`
`
`




`
`
`


word
 
永和
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
0
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
2


word
 
服装
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
4


word
 
饰品
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
4
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
6


word
 
有限
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
8


word
 
公司
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
8
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
10


word
 
有限
公司
有限公司
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
10


`
`
`






7
.
 
ChineseAnalyzer
 
for
 
Whoosh
 
搜索
索引
引擎
搜索引擎


--------------------------------------------


*
 
引用
：
 
`
from
 
jieba
.
analyse
 
import
 
ChineseAnalyzer
`


*
 
用法
示例
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
test
_
whoosh
.
py




8
.
 
命令
命令行
分词


-------------------




使用
示例
：
`
python
 
-
m
 
jieba
 
news
.
txt
 
>
 
cut
_
result
.
txt
`




命令
命令行
选项
（
翻译
）
：




 
 
 
 
使用
:
 
python
 
-
m
 
jieba
 
[
options
]
 
filename




 
 
 
 
结巴
命令
命令行
界面
。




 
 
 
 
固定
参数
:


 
 
 
 
 
 
filename
 
 
 
 
 
 
 
 
 
 
 
 
 
 
输入
文件




 
 
 
 
可
选
参数
:


 
 
 
 
 
 
-
h
,
 
--
help
 
 
 
 
 
 
 
 
 
 
 
 
显示
此
帮助
信息
并
退出


 
 
 
 
 
 
-
d
 
[
DELIM
]
,
 
--
delimiter
 
[
DELIM
]


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
使用
 
DELIM
 
分隔
词语
，
而
不是
用
默认
的
'
 
/
 
'
。


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
若
不
指定
 
DELIM
，
则
使用
一个
空格
分隔
。


 
 
 
 
 
 
-
p
 
[
DELIM
]
,
 
--
pos
 
[
DELIM
]


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
启用
词性
标注
；
如果
指定
 
DELIM
，
词语
和
词性
之间


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
用
它
分隔
，
否则
用
 
_
 
分隔


 
 
 
 
 
 
-
D
 
DICT
,
 
--
dict
 
DICT
 
 
使用
 
DICT
 
代替
默认
词典


 
 
 
 
 
 
-
u
 
USER
_
DICT
,
 
--
user
-
dict
 
USER
_
DICT


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
使用
 
USER
_
DICT
 
作为
附加
词典
，
与
默认
词典
或
自定
定义
自定义
词典
配合
使用


 
 
 
 
 
 
-
a
,
 
--
cut
-
all
 
 
 
 
 
 
 
 
 
全
模式
分词
（
不
支持
词性
标注
）


 
 
 
 
 
 
-
n
,
 
--
no
-
hmm
 
 
 
 
 
 
 
 
 
 
不
使用
隐含
可夫
马尔可
马尔可夫
模型


 
 
 
 
 
 
-
q
,
 
--
quiet
 
 
 
 
 
 
 
 
 
 
 
不
输出
载入
信息
到
 
STDERR


 
 
 
 
 
 
-
V
,
 
--
version
 
 
 
 
 
 
 
 
 
显示
版本
信息
版本信息
并
退出




 
 
 
 
如果
没有
指定
文件
文件名
，
则
使用
标准
输入
。




`
--
help
`
 
选项
输出
：




 
 
 
 
$
>
 
python
 
-
m
 
jieba
 
--
help


 
 
 
 
Jieba
 
command
 
line
 
interface
.




 
 
 
 
positional
 
arguments
:


 
 
 
 
 
 
filename
 
 
 
 
 
 
 
 
 
 
 
 
 
 
input
 
file




 
 
 
 
optional
 
arguments
:


 
 
 
 
 
 
-
h
,
 
--
help
 
 
 
 
 
 
 
 
 
 
 
 
show
 
this
 
help
 
message
 
and
 
exit


 
 
 
 
 
 
-
d
 
[
DELIM
]
,
 
--
delimiter
 
[
DELIM
]


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
use
 
DELIM
 
instead
 
of
 
'
 
/
 
'
 
for
 
word
 
delimiter
;
 
or
 
a


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
space
 
if
 
it
 
is
 
used
 
without
 
DELIM


 
 
 
 
 
 
-
p
 
[
DELIM
]
,
 
--
pos
 
[
DELIM
]


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
enable
 
POS
 
tagging
;
 
if
 
DELIM
 
is
 
specified
,
 
use
 
DELIM


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
instead
 
of
 
'
_
'
 
for
 
POS
 
delimiter


 
 
 
 
 
 
-
D
 
DICT
,
 
--
dict
 
DICT
 
 
use
 
DICT
 
as
 
dictionary


 
 
 
 
 
 
-
u
 
USER
_
DICT
,
 
--
user
-
dict
 
USER
_
DICT


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
use
 
USER
_
DICT
 
together
 
with
 
the
 
default
 
dictionary
 
or


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
DICT
 
(
if
 
specified
)


 
 
 
 
 
 
-
a
,
 
--
cut
-
all
 
 
 
 
 
 
 
 
 
full
 
pattern
 
cutting
 
(
ignored
 
with
 
POS
 
tagging
)


 
 
 
 
 
 
-
n
,
 
--
no
-
hmm
 
 
 
 
 
 
 
 
 
 
don
'
t
 
use
 
the
 
Hidden
 
Markov
 
Model


 
 
 
 
 
 
-
q
,
 
--
quiet
 
 
 
 
 
 
 
 
 
 
 
don
'
t
 
print
 
loading
 
messages
 
to
 
stderr


 
 
 
 
 
 
-
V
,
 
--
version
 
 
 
 
 
 
 
 
 
show
 
program
'
s
 
version
 
number
 
and
 
exit




 
 
 
 
If
 
no
 
filename
 
specified
,
 
use
 
STDIN
 
instead
.




延迟
加载
机制


------------




jieba
 
采用
延迟
加载
，
`
import
 
jieba
`
 
和
 
`
jieba
.
Tokenizer
(
)
`
 
不会
立即
触发
词典
的
加载
，
一旦
有
必要
才
开始
加载
词典
构建
前缀
字典
。
如果
你
想
手工
初始
 
jieba
，
也
可以
手动
初始
初始化
。




 
 
 
 
import
 
jieba


 
 
 
 
jieba
.
initialize
(
)
 
 
#
 
手动
初始
初始化
（
可
选
）






在
 
0.28
 
之前
的
版本
是
不能
指定
主
词典
的
路径
的
，
有
了
延迟
加载
机制
后
，
你
可以
改变
主
词典
的
路径
:




 
 
 
 
jieba
.
set
_
dictionary
(
'
data
/
dict
.
txt
.
big
'
)




例子
：
 
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
test
_
change
_
dictpath
.
py




其他
词典


=
=
=
=
=
=
=
=


1
.
 
占用
内存
较
小
的
词典
文件


https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
raw
/
master
/
extra
_
dict
/
dict
.
txt
.
small




2
.
 
支持
繁体
分词
更好
的
词典
文件


https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
raw
/
master
/
extra
_
dict
/
dict
.
txt
.
big




下载
你
所
需要
的
词典
，
然后
覆盖
 
jieba
/
dict
.
txt
 
即可
；
或者
用
 
`
jieba
.
set
_
dictionary
(
'
data
/
dict
.
txt
.
big
'
)
`




其他
语言
实现


=
=
=
=
=
=
=
=
=
=




结巴
分词
 
Java
 
版本


----------------


作者
：
piaolingxue


地址
：
https
:
/
/
github
.
com
/
huaban
/
jieba
-
analysis




结巴
分词
 
C++
 
版本


----------------


作者
：
yanyiwu


地址
：
https
:
/
/
github
.
com
/
yanyiwu
/
cppjieba




结巴
分词
 
Rust
 
版本


----------------


作者
：
messense
,
 
MnO2


地址
：
https
:
/
/
github
.
com
/
messense
/
jieba
-
rs




结巴
分词
 
Node
.
js
 
版本


----------------


作者
：
yanyiwu


地址
：
https
:
/
/
github
.
com
/
yanyiwu
/
nodejieba




结巴
分词
 
Erlang
 
版本


----------------


作者
：
falood


地址
：
https
:
/
/
github
.
com
/
falood
/
exjieba




结巴
分词
 
R
 
版本


----------------


作者
：
qinwf


地址
：
https
:
/
/
github
.
com
/
qinwf
/
jiebaR




结巴
分词
 
iOS
 
版本


----------------


作者
：
yanyiwu


地址
：
https
:
/
/
github
.
com
/
yanyiwu
/
iosjieba




结巴
分词
 
PHP
 
版本


----------------


作者
：
fukuball


地址
：
https
:
/
/
github
.
com
/
fukuball
/
jieba
-
php




结巴
分词
 
.
NET
(
C#
)
 
版本


----------------


作者
：
anderscui


地址
：
https
:
/
/
github
.
com
/
anderscui
/
jieba
.
NET
/




结巴
分词
 
Go
 
版本


----------------




+
 
作者
:
 
wangbin
 
地址
:
 
https
:
/
/
github
.
com
/
wangbin
/
jiebago


+
 
作者
:
 
yanyiwu
 
地址
:
 
https
:
/
/
github
.
com
/
yanyiwu
/
gojieba




结巴
分词
Android
版本


------------------


+
 
作者
 
 
 
Dongliang
.
W
 
 
地址
：
https
:
/
/
github
.
com
/
452896915
/
jieba
-
android






友情
链接
友情链接


=
=
=
=
=
=
=
=
=


*
 
https
:
/
/
github
.
com
/
baidu
/
lac
 
 
 
百度
中文
词法
分析
（
分词
+
词性
+
专名
）
系统


*
 
https
:
/
/
github
.
com
/
baidu
/
AnyQ
 
 
百度
FAQ
自动
问答
系统


*
 
https
:
/
/
github
.
com
/
baidu
/
Senta
 
百度
情感
识别
系统
识别系统




系统
集成
系统集成


=
=
=
=
=
=
=
=


1
.
 
Solr
:
 
https
:
/
/
github
.
com
/
sing1ee
/
jieba
-
solr




分词
速度


=
=
=
=
=
=
=
=
=


*
 
1.5
 
MB
 
/
 
Second
 
in
 
Full
 
Mode


*
 
400
 
KB
 
/
 
Second
 
in
 
Default
 
Mode


*
 
测试
环境
测试环境
:
 
Intel
(
R
)
 
Core
(
TM
)
 
i7
-
2600
 
CPU
 
@
 
3.4
GHz
；
《
围城
》
.
txt




常见
问题
常见问题


=
=
=
=
=
=
=
=
=




##
 
1
.
 
模型
的
数据
是
如何
生成
的
？




详见
：
 
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
issues
/
7




##
 
2
.
 
“
台
中
”
总是
被
切成
“
台
 
中
”
？
（
以及
类似
情况
）




P
(
台
中
)
 
＜
 
P
(
台
)
×
P
(
中
)
，
“
台
中
”
词频
不够
导致
其成
词
概率
较
低




解决
方法
：
强制
调
高
词频




`
jieba
.
add
_
word
(
'
台
中
'
)
`
 
或者
 
`
jieba
.
suggest
_
freq
(
'
台
中
'
,
 
True
)
`




##
 
3
.
 
“
今天
天天
天气
今天天气
 
不错
”
应该
被
切成
“
今天
 
天气
 
不错
”
？
（
以及
类似
情况
）




解决
方法
：
强制
调低
词频




`
jieba
.
suggest
_
freq
(
(
'
今天
'
,
 
'
天气
'
)
,
 
True
)
`




或者
直接
删除
该词
 
`
jieba
.
del
_
word
(
'
今天
天天
天气
今天天气
'
)
`




##
 
4
.
 
切出
了
词典
中
没有
的
词语
，
效果
不
理想
？




解决
方法
：
关闭
新词
发现




`
jieba
.
cut
(
'
丰田
太省
了
'
,
 
HMM
=
False
)
`


`
jieba
.
cut
(
'
我们
中出
了
一个
叛徒
'
,
 
HMM
=
False
)
`




*
*
更
多
问题
请
点击
*
*
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
issues
?
sort
=
updated
&
state
=
closed




修订
历史


=
=
=
=
=
=
=
=
=
=


https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
Changelog




--------------------




jieba


=
=
=
=
=
=
=
=


"
Jieba
"
 
(
Chinese
 
for
 
"
to
 
stutter
"
)
 
Chinese
 
text
 
segmentation
:
 
built
 
to
 
be
 
the
 
best
 
Python
 
Chinese
 
word
 
segmentation
 
module
.




Features


=
=
=
=
=
=
=
=


*
 
Support
 
three
 
types
 
of
 
segmentation
 
mode
:




1
.
 
Accurate
 
Mode
 
attempts
 
to
 
cut
 
the
 
sentence
 
into
 
the
 
most
 
accurate
 
segmentations
,
 
which
 
is
 
suitable
 
for
 
text
 
analysis
.


2
.
 
Full
 
Mode
 
gets
 
all
 
the
 
possible
 
words
 
from
 
the
 
sentence
.
 
Fast
 
but
 
not
 
accurate
.


3
.
 
Search
 
Engine
 
Mode
,
 
based
 
on
 
the
 
Accurate
 
Mode
,
 
attempts
 
to
 
cut
 
long
 
words
 
into
 
several
 
short
 
words
,
 
which
 
can
 
raise
 
the
 
recall
 
rate
.
 
Suitable
 
for
 
search
 
engines
.




*
 
Supports
 
Traditional
 
Chinese


*
 
Supports
 
customized
 
dictionaries


*
 
MIT
 
License






Online
 
demo


=
=
=
=
=
=
=
=
=


http
:
/
/
jiebademo
.
ap01
.
aws
.
af
.
cm
/




(
Powered
 
by
 
Appfog
)




Usage


=
=
=
=
=
=
=
=


*
 
Fully
 
automatic
 
installation
:
 
`
easy
_
install
 
jieba
`
 
or
 
`
pip
 
install
 
jieba
`


*
 
Semi
-
automatic
 
installation
:
 
Download
 
http
:
/
/
pypi
.
python
.
org
/
pypi
/
jieba
/
 
,
 
run
 
`
python
 
setup
.
py
 
install
`
 
after
 
extracting
.


*
 
Manual
 
installation
:
 
place
 
the
 
`
jieba
`
 
directory
 
in
 
the
 
current
 
directory
 
or
 
python
 
`
site
-
packages
`
 
directory
.


*
 
`
import
 
jieba
`
.




Algorithm


=
=
=
=
=
=
=
=


*
 
Based
 
on
 
a
 
prefix
 
dictionary
 
structure
 
to
 
achieve
 
efficient
 
word
 
graph
 
scanning
.
 
Build
 
a
 
directed
 
acyclic
 
graph
 
(
DAG
)
 
for
 
all
 
possible
 
word
 
combinations
.


*
 
Use
 
dynamic
 
programming
 
to
 
find
 
the
 
most
 
probable
 
combination
 
based
 
on
 
the
 
word
 
frequency
.


*
 
For
 
unknown
 
words
,
 
a
 
HMM
-
based
 
model
 
is
 
used
 
with
 
the
 
Viterbi
 
algorithm
.




Main
 
Functions


=
=
=
=
=
=
=
=
=
=
=
=
=
=




1
.
 
Cut


--------


*
 
The
 
`
jieba
.
cut
`
 
function
 
accepts
 
three
 
input
 
parameters
:
 
the
 
first
 
parameter
 
is
 
the
 
string
 
to
 
be
 
cut
;
 
the
 
second
 
parameter
 
is
 
`
cut
_
all
`
,
 
controlling
 
the
 
cut
 
mode
;
 
the
 
third
 
parameter
 
is
 
to
 
control
 
whether
 
to
 
use
 
the
 
Hidden
 
Markov
 
Model
.


*
 
`
jieba
.
cut
_
for
_
search
`
 
accepts
 
two
 
parameter
:
 
the
 
string
 
to
 
be
 
cut
;
 
whether
 
to
 
use
 
the
 
Hidden
 
Markov
 
Model
.
 
This
 
will
 
cut
 
the
 
sentence
 
into
 
short
 
words
 
suitable
 
for
 
search
 
engines
.


*
 
The
 
input
 
string
 
can
 
be
 
an
 
unicode
/
str
 
object
,
 
or
 
a
 
str
/
bytes
 
object
 
which
 
is
 
encoded
 
in
 
UTF
-
8
 
or
 
GBK
.
 
Note
 
that
 
using
 
GBK
 
encoding
 
is
 
not
 
recommended
 
because
 
it
 
may
 
be
 
unexpectly
 
decoded
 
as
 
UTF
-
8
.


*
 
`
jieba
.
cut
`
 
and
 
`
jieba
.
cut
_
for
_
search
`
 
returns
 
an
 
generator
,
 
from
 
which
 
you
 
can
 
use
 
a
 
`
for
`
 
loop
 
to
 
get
 
the
 
segmentation
 
result
 
(
in
 
unicode
)
.


*
 
`
jieba
.
lcut
`
 
and
 
`
jieba
.
lcut
_
for
_
search
`
 
returns
 
a
 
list
.


*
 
`
jieba
.
Tokenizer
(
dictionary
=
DEFAULT
_
DICT
)
`
 
creates
 
a
 
new
 
customized
 
Tokenizer
,
 
which
 
enables
 
you
 
to
 
use
 
different
 
dictionaries
 
at
 
the
 
same
 
time
.
 
`
jieba
.
dt
`
 
is
 
the
 
default
 
Tokenizer
,
 
to
 
which
 
almost
 
all
 
global
 
functions
 
are
 
mapped
.






*
*
Code
 
example
:
 
segmentation
*
*




`
`
`
python


#
encoding
=
utf
-
8


import
 
jieba




seg
_
list
 
=
 
jieba
.
cut
(
"
我
来到
北京
清华
华大
大学
清华大学
"
,
 
cut
_
all
=
True
)


print
(
"
Full
 
Mode
:
 
"
 
+
 
"
/
 
"
.
join
(
seg
_
list
)
)
 
 
#
 
全
模式




seg
_
list
 
=
 
jieba
.
cut
(
"
我
来到
北京
清华
华大
大学
清华大学
"
,
 
cut
_
all
=
False
)


print
(
"
Default
 
Mode
:
 
"
 
+
 
"
/
 
"
.
join
(
seg
_
list
)
)
 
 
#
 
默认
模式




seg
_
list
 
=
 
jieba
.
cut
(
"
他
来到
了
网易
杭研
大厦
"
)


print
(
"
,
 
"
.
join
(
seg
_
list
)
)




seg
_
list
 
=
 
jieba
.
cut
_
for
_
search
(
"
小明
硕士
毕业
于
中国
科学
学院
科学院
中国科学院
计算
计算所
，
后
在
日本
京都
大学
日本京都大学
深造
"
)
 
 
#
 
搜索
索引
引擎
搜索引擎
模式


print
(
"
,
 
"
.
join
(
seg
_
list
)
)


`
`
`




Output
:




 
 
 
 
[
Full
 
Mode
]
:
 
我
/
 
来到
/
 
北京
/
 
清华
/
 
清华
华大
大学
清华大学
/
 
华大
/
 
大学




 
 
 
 
[
Accurate
 
Mode
]
:
 
我
/
 
来到
/
 
北京
/
 
清华
华大
大学
清华大学




 
 
 
 
[
Unknown
 
Words
 
Recognize
]
 
他
,
 
来到
,
 
了
,
 
网易
,
 
杭研
,
 
大厦
 
 
 
 
(
In
 
this
 
case
,
 
"
杭研
"
 
is
 
not
 
in
 
the
 
dictionary
,
 
but
 
is
 
identified
 
by
 
the
 
Viterbi
 
algorithm
)




 
 
 
 
[
Search
 
Engine
 
Mode
]
：
 
小明
,
 
硕士
,
 
毕业
,
 
于
,
 
中国
,
 
科学
,
 
学院
,
 
科学
学院
科学院
,
 
中国
科学
学院
科学院
中国科学院
,
 
计算
,
 
计算
计算所
,
 
后
,
 
在
,
 
日本
,
 
京都
,
 
大学
,
 
日本
京都
大学
日本京都大学
,
 
深造






2
.
 
Add
 
a
 
custom
 
dictionary


----------------------------




###
 
Load
 
dictionary




*
 
Developers
 
can
 
specify
 
their
 
own
 
custom
 
dictionary
 
to
 
be
 
included
 
in
 
the
 
jieba
 
default
 
dictionary
.
 
Jieba
 
is
 
able
 
to
 
identify
 
new
 
words
,
 
but
 
you
 
can
 
add
 
your
 
own
 
new
 
words
 
can
 
ensure
 
a
 
higher
 
accuracy
.


*
 
Usage
：
 
`
jieba
.
load
_
userdict
(
file
_
name
)
`
 
#
 
file
_
name
 
is
 
a
 
file
-
like
 
object
 
or
 
the
 
path
 
of
 
the
 
custom
 
dictionary


*
 
The
 
dictionary
 
format
 
is
 
the
 
same
 
as
 
that
 
of
 
`
dict
.
txt
`
:
 
one
 
word
 
per
 
line
;
 
each
 
line
 
is
 
divided
 
into
 
three
 
parts
 
separated
 
by
 
a
 
space
:
 
word
,
 
word
 
frequency
,
 
POS
 
tag
.
 
If
 
`
file
_
name
`
 
is
 
a
 
path
 
or
 
a
 
file
 
opened
 
in
 
binary
 
mode
,
 
the
 
dictionary
 
must
 
be
 
UTF
-
8
 
encoded
.


*
 
The
 
word
 
frequency
 
and
 
POS
 
tag
 
can
 
be
 
omitted
 
respectively
.
 
The
 
word
 
frequency
 
will
 
be
 
filled
 
with
 
a
 
suitable
 
value
 
if
 
omitted
.




*
*
For
 
example
:
*
*




`
`
`


创新
办
 
3
 
i


云
计算
 
5


凱特琳
 
nz


台
中


`
`
`






*
 
Change
 
a
 
Tokenizer
'
s
 
`
tmp
_
dir
`
 
and
 
`
cache
_
file
`
 
to
 
specify
 
the
 
path
 
of
 
the
 
cache
 
file
,
 
for
 
using
 
on
 
a
 
restricted
 
file
 
system
.




*
 
Example
:




 
 
 
 
 
 
 
 
云
计算
 
5


 
 
 
 
 
 
 
 
李小福
 
2


 
 
 
 
 
 
 
 
创新
办
 
3




 
 
 
 
 
 
 
 
[
Before
]
：
 
李小福
 
/
 
是
 
/
 
创新
 
/
 
办
 
/
 
主任
 
/
 
也
 
/
 
是
 
/
 
云
 
/
 
计算
 
/
 
方面
 
/
 
的
 
/
 
专家
 
/




 
 
 
 
 
 
 
 
[
After
]
：
　
李小福
 
/
 
是
 
/
 
创新
办
 
/
 
主任
 
/
 
也
 
/
 
是
 
/
 
云
计算
 
/
 
方面
 
/
 
的
 
/
 
专家
 
/






###
 
Modify
 
dictionary




*
 
Use
 
`
add
_
word
(
word
,
 
freq
=
None
,
 
tag
=
None
)
`
 
and
 
`
del
_
word
(
word
)
`
 
to
 
modify
 
the
 
dictionary
 
dynamically
 
in
 
programs
.


*
 
Use
 
`
suggest
_
freq
(
segment
,
 
tune
=
True
)
`
 
to
 
adjust
 
the
 
frequency
 
of
 
a
 
single
 
word
 
so
 
that
 
it
 
can
 
(
or
 
cannot
)
 
be
 
segmented
.




*
 
Note
 
that
 
HMM
 
may
 
affect
 
the
 
final
 
result
.




Example
:




`
`
`
pycon


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
如果
放到
post
中将
出错
。
'
,
 
HMM
=
False
)
)
)


如果
/
放到
/
post
/
中将
/
出错
/
。


>
>
>
 
jieba
.
suggest
_
freq
(
(
'
中
'
,
 
'
将
'
)
,
 
True
)


494


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
如果
放到
post
中将
出错
。
'
,
 
HMM
=
False
)
)
)


如果
/
放到
/
post
/
中
/
将
/
出错
/
。


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
「
台
中
」
正确
应该
不会
被
切开
'
,
 
HMM
=
False
)
)
)


「
/
台
/
中
/
」
/
正确
/
应该
/
不会
/
被
/
切开


>
>
>
 
jieba
.
suggest
_
freq
(
'
台
中
'
,
 
True
)


69


>
>
>
 
print
(
'
/
'
.
join
(
jieba
.
cut
(
'
「
台
中
」
正确
应该
不会
被
切开
'
,
 
HMM
=
False
)
)
)


「
/
台
中
/
」
/
正确
/
应该
/
不会
/
被
/
切开


`
`
`




3
.
 
Keyword
 
Extraction


-----------------------


`
import
 
jieba
.
analyse
`




*
 
`
jieba
.
analyse
.
extract
_
tags
(
sentence
,
 
topK
=
20
,
 
withWeight
=
False
,
 
allowPOS
=
(
)
)
`


 
 
*
 
`
sentence
`
:
 
the
 
text
 
to
 
be
 
extracted


 
 
*
 
`
topK
`
:
 
return
 
how
 
many
 
keywords
 
with
 
the
 
highest
 
TF
/
IDF
 
weights
.
 
The
 
default
 
value
 
is
 
20


 
 
*
 
`
withWeight
`
:
 
whether
 
return
 
TF
/
IDF
 
weights
 
with
 
the
 
keywords
.
 
The
 
default
 
value
 
is
 
False


 
 
*
 
`
allowPOS
`
:
 
filter
 
words
 
with
 
which
 
POSs
 
are
 
included
.
 
Empty
 
for
 
no
 
filtering
.


*
 
`
jieba
.
analyse
.
TFIDF
(
idf
_
path
=
None
)
`
 
creates
 
a
 
new
 
TFIDF
 
instance
,
 
`
idf
_
path
`
 
specifies
 
IDF
 
file
 
path
.




Example
 
(
keyword
 
extraction
)




https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
.
py




Developers
 
can
 
specify
 
their
 
own
 
custom
 
IDF
 
corpus
 
in
 
jieba
 
keyword
 
extraction




*
 
Usage
：
 
`
jieba
.
analyse
.
set
_
idf
_
path
(
file
_
name
)
 
#
 
file
_
name
 
is
 
the
 
path
 
for
 
the
 
custom
 
corpus
`


*
 
Custom
 
Corpus
 
Sample
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
extra
_
dict
/
idf
.
txt
.
big


*
 
Sample
 
Code
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
_
idfpath
.
py




Developers
 
can
 
specify
 
their
 
own
 
custom
 
stop
 
words
 
corpus
 
in
 
jieba
 
keyword
 
extraction




*
 
Usage
：
 
`
jieba
.
analyse
.
set
_
stop
_
words
(
file
_
name
)
 
#
 
file
_
name
 
is
 
the
 
path
 
for
 
the
 
custom
 
corpus
`


*
 
Custom
 
Corpus
 
Sample
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
extra
_
dict
/
stop
_
words
.
txt


*
 
Sample
 
Code
：
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
extract
_
tags
_
stop
_
words
.
py




There
'
s
 
also
 
a
 
[
TextRank
]
(
http
:
/
/
web
.
eecs
.
umich
.
edu
/
~
mihalcea
/
papers
/
mihalcea
.
emnlp04
.
pdf
)
 
implementation
 
available
.




Use
:
 
`
jieba
.
analyse
.
textrank
(
sentence
,
 
topK
=
20
,
 
withWeight
=
False
,
 
allowPOS
=
(
'
ns
'
,
 
'
n
'
,
 
'
vn
'
,
 
'
v
'
)
)
`




Note
 
that
 
it
 
filters
 
POS
 
by
 
default
.




`
jieba
.
analyse
.
TextRank
(
)
`
 
creates
 
a
 
new
 
TextRank
 
instance
.




4
.
 
Part
 
of
 
Speech
 
Tagging


-------------------------


*
 
`
jieba
.
posseg
.
POSTokenizer
(
tokenizer
=
None
)
`
 
creates
 
a
 
new
 
customized
 
Tokenizer
.
 
`
tokenizer
`
 
specifies
 
the
 
jieba
.
Tokenizer
 
to
 
internally
 
use
.
 
`
jieba
.
posseg
.
dt
`
 
is
 
the
 
default
 
POSTokenizer
.


*
 
Tags
 
the
 
POS
 
of
 
each
 
word
 
after
 
segmentation
,
 
using
 
labels
 
compatible
 
with
 
ictclas
.


*
 
Example
:




`
`
`
pycon


>
>
>
 
import
 
jieba
.
posseg
 
as
 
pseg


>
>
>
 
words
 
=
 
pseg
.
cut
(
"
我
爱
北京
天安
天安门
"
)


>
>
>
 
for
 
w
 
in
 
words
:


...
 
 
 
 
print
(
'
%
s
 
%
s
'
 
%
 
(
w
.
word
,
 
w
.
flag
)
)


...


我
 
r


爱
 
v


北京
 
ns


天安
天安门
 
ns


`
`
`




5
.
 
Parallel
 
Processing


----------------------


*
 
Principle
:
 
Split
 
target
 
text
 
by
 
line
,
 
assign
 
the
 
lines
 
into
 
multiple
 
Python
 
processes
,
 
and
 
then
 
merge
 
the
 
results
,
 
which
 
is
 
considerably
 
faster
.


*
 
Based
 
on
 
the
 
multiprocessing
 
module
 
of
 
Python
.


*
 
Usage
:


 
 
 
 
*
 
`
jieba
.
enable
_
parallel
(
4
)
`
 
#
 
Enable
 
parallel
 
processing
.
 
The
 
parameter
 
is
 
the
 
number
 
of
 
processes
.


 
 
 
 
*
 
`
jieba
.
disable
_
parallel
(
)
`
 
#
 
Disable
 
parallel
 
processing
.




*
 
Example
:


 
 
 
 
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
parallel
/
test
_
file
.
py




*
 
Result
:
 
On
 
a
 
four
-
core
 
3.4
GHz
 
Linux
 
machine
,
 
do
 
accurate
 
word
 
segmentation
 
on
 
Complete
 
Works
 
of
 
Jin
 
Yong
,
 
and
 
the
 
speed
 
reaches
 
1MB
/
s
,
 
which
 
is
 
3.3
 
times
 
faster
 
than
 
the
 
single
-
process
 
version
.




*
 
*
*
Note
*
*
 
that
 
parallel
 
processing
 
supports
 
only
 
default
 
tokenizers
,
 
`
jieba
.
dt
`
 
and
 
`
jieba
.
posseg
.
dt
`
.




6
.
 
Tokenize
:
 
return
 
words
 
with
 
position


----------------------------------------


*
 
The
 
input
 
must
 
be
 
unicode


*
 
Default
 
mode




`
`
`
python


result
 
=
 
jieba
.
tokenize
(
u
'
永和
服装
饰品
有限
公司
有限公司
'
)


for
 
tk
 
in
 
result
:


 
 
 
 
print
(
"
word
 
%
s
\
t
\
t
 
start
:
 
%
d
 
\
t
\
t
 
end
:
%
d
"
 
%
 
(
tk
[
0
]
,
tk
[
1
]
,
tk
[
2
]
)
)


`
`
`




`
`
`


word
 
永和
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
0
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
2


word
 
服装
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
4


word
 
饰品
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
4
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
6


word
 
有限
公司
有限公司
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
10




`
`
`




*
 
Search
 
mode




`
`
`
python


result
 
=
 
jieba
.
tokenize
(
u
'
永和
服装
饰品
有限
公司
有限公司
'
,
mode
=
'
search
'
)


for
 
tk
 
in
 
result
:


 
 
 
 
print
(
"
word
 
%
s
\
t
\
t
 
start
:
 
%
d
 
\
t
\
t
 
end
:
%
d
"
 
%
 
(
tk
[
0
]
,
tk
[
1
]
,
tk
[
2
]
)
)


`
`
`




`
`
`


word
 
永和
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
0
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
2


word
 
服装
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
4


word
 
饰品
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
4
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
6


word
 
有限
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
8


word
 
公司
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
8
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
10


word
 
有限
公司
有限公司
 
 
 
 
 
 
 
 
 
 
 
 
start
:
 
6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
end
:
10


`
`
`






7
.
 
ChineseAnalyzer
 
for
 
Whoosh


-------------------------------


*
 
`
from
 
jieba
.
analyse
 
import
 
ChineseAnalyzer
`


*
 
Example
:
 
https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
blob
/
master
/
test
/
test
_
whoosh
.
py




8
.
 
Command
 
Line
 
Interface


--------------------------------




 
 
 
 
$
>
 
python
 
-
m
 
jieba
 
--
help


 
 
 
 
Jieba
 
command
 
line
 
interface
.




 
 
 
 
positional
 
arguments
:


 
 
 
 
 
 
filename
 
 
 
 
 
 
 
 
 
 
 
 
 
 
input
 
file




 
 
 
 
optional
 
arguments
:


 
 
 
 
 
 
-
h
,
 
--
help
 
 
 
 
 
 
 
 
 
 
 
 
show
 
this
 
help
 
message
 
and
 
exit


 
 
 
 
 
 
-
d
 
[
DELIM
]
,
 
--
delimiter
 
[
DELIM
]


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
use
 
DELIM
 
instead
 
of
 
'
 
/
 
'
 
for
 
word
 
delimiter
;
 
or
 
a


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
space
 
if
 
it
 
is
 
used
 
without
 
DELIM


 
 
 
 
 
 
-
p
 
[
DELIM
]
,
 
--
pos
 
[
DELIM
]


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
enable
 
POS
 
tagging
;
 
if
 
DELIM
 
is
 
specified
,
 
use
 
DELIM


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
instead
 
of
 
'
_
'
 
for
 
POS
 
delimiter


 
 
 
 
 
 
-
D
 
DICT
,
 
--
dict
 
DICT
 
 
use
 
DICT
 
as
 
dictionary


 
 
 
 
 
 
-
u
 
USER
_
DICT
,
 
--
user
-
dict
 
USER
_
DICT


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
use
 
USER
_
DICT
 
together
 
with
 
the
 
default
 
dictionary
 
or


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
DICT
 
(
if
 
specified
)


 
 
 
 
 
 
-
a
,
 
--
cut
-
all
 
 
 
 
 
 
 
 
 
full
 
pattern
 
cutting
 
(
ignored
 
with
 
POS
 
tagging
)


 
 
 
 
 
 
-
n
,
 
--
no
-
hmm
 
 
 
 
 
 
 
 
 
 
don
'
t
 
use
 
the
 
Hidden
 
Markov
 
Model


 
 
 
 
 
 
-
q
,
 
--
quiet
 
 
 
 
 
 
 
 
 
 
 
don
'
t
 
print
 
loading
 
messages
 
to
 
stderr


 
 
 
 
 
 
-
V
,
 
--
version
 
 
 
 
 
 
 
 
 
show
 
program
'
s
 
version
 
number
 
and
 
exit




 
 
 
 
If
 
no
 
filename
 
specified
,
 
use
 
STDIN
 
instead
.




Initialization


---------------


By
 
default
,
 
Jieba
 
don
'
t
 
build
 
the
 
prefix
 
dictionary
 
unless
 
it
'
s
 
necessary
.
 
This
 
takes
 
1
-
3
 
seconds
,
 
after
 
which
 
it
 
is
 
not
 
initialized
 
again
.
 
If
 
you
 
want
 
to
 
initialize
 
Jieba
 
manually
,
 
you
 
can
 
call
:




 
 
 
 
import
 
jieba


 
 
 
 
jieba
.
initialize
(
)
 
 
#
 
(
optional
)




You
 
can
 
also
 
specify
 
the
 
dictionary
 
(
not
 
supported
 
before
 
version
 
0.28
)
 
:




 
 
 
 
jieba
.
set
_
dictionary
(
'
data
/
dict
.
txt
.
big
'
)






Using
 
Other
 
Dictionaries


=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=




It
 
is
 
possible
 
to
 
use
 
your
 
own
 
dictionary
 
with
 
Jieba
,
 
and
 
there
 
are
 
also
 
two
 
dictionaries
 
ready
 
for
 
download
:




1
.
 
A
 
smaller
 
dictionary
 
for
 
a
 
smaller
 
memory
 
footprint
:


https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
raw
/
master
/
extra
_
dict
/
dict
.
txt
.
small




2
.
 
There
 
is
 
also
 
a
 
bigger
 
dictionary
 
that
 
has
 
better
 
support
 
for
 
traditional
 
Chinese
 
(
繁體
)
:


https
:
/
/
github
.
com
/
fxsjy
/
jieba
/
raw
/
master
/
extra
_
dict
/
dict
.
txt
.
big




By
 
default
,
 
an
 
in
-
between
 
dictionary
 
is
 
used
,
 
called
 
`
dict
.
txt
`
 
and
 
included
 
in
 
the
 
distribution
.




In
 
either
 
case
,
 
download
 
the
 
file
 
you
 
want
,
 
and
 
then
 
call
 
`
jieba
.
set
_
dictionary
(
'
data
/
dict
.
txt
.
big
'
)
`
 
or
 
just
 
replace
 
the
 
existing
 
`
dict
.
txt
`
.




Segmentation
 
speed


=
=
=
=
=
=
=
=
=


*
 
1.5
 
MB
 
/
 
Second
 
in
 
Full
 
Mode


*
 
400
 
KB
 
/
 
Second
 
in
 
Default
 
Mode


*
 
Test
 
Env
:
 
Intel
(
R
)
 
Core
(
TM
)
 
i7
-
2600
 
CPU
 
@
 
3.4
GHz
；
《
围城
》
.
txt




